{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Fraud Reviews: Exploratory Data Analysis and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# Pre=Processing\n",
    "import re\n",
    "import nltk\n",
    "import demoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV file\n",
    "fake_reviews_path = '/Users/ShanShan/Fake-Reviews-Detection/Dataset/fake reviews dataset.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "fake_reviews_df = pd.read_csv(fake_reviews_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "fake_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Excel file\n",
    "yelp_path = '/Users/ShanShan/Fake-Reviews-Detection/Dataset/Yelp Labelled Review Dataset with Sentiments and Features.xlsx'\n",
    "\n",
    "# Read the Excel file\n",
    "yelp_df = pd.read_excel(yelp_path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "fake_reviews_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "yelp_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for duplicated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_df_duplicates = fake_reviews_df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicates in dataset:\", fake_reviews_df_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the actual duplicate rows\n",
    "fake_reviews_duplicate_rows = fake_reviews_df[fake_reviews_df.duplicated(keep=False)]\n",
    "\n",
    "print(\"Duplicate rows in the dataset:\\n\", fake_reviews_duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates while keeping the first occurrence\n",
    "fake_reviews_df = fake_reviews_df.drop_duplicates()\n",
    "\n",
    "# Check the new count of duplicates to confirm removal\n",
    "print(\"Number of duplicates after dropping: \", fake_reviews_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df_duplicates = yelp_df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicates in dataset: \", yelp_df_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot for Fake Reviews countplot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Choose a color palette from Set1 with a fixed number of colors\n",
    "set1_colors = sns.color_palette(\"Set1\", n_colors=5)\n",
    "\n",
    "# Define a mapping of ratings to colors from the Set1 palette\n",
    "rating_colors_fake_reviews = {\n",
    "    1.0: set1_colors[0],  # First color (Red)\n",
    "    2.0: set1_colors[1],  # Second color (Blue)\n",
    "    3.0: set1_colors[2],  # Third color (Green)\n",
    "    4.0: set1_colors[3],  # Fourth color (Orange)\n",
    "    5.0: set1_colors[4]   # Fifth color (Yellow)\n",
    "}\n",
    "\n",
    "# Plot the countplot with hue set to 'rating'\n",
    "ax_fake_reviews = plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "sns.countplot(x='rating', data=fake_reviews_df, hue='rating', palette=rating_colors_fake_reviews)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of Ratings (Fake Reviews Dataset)\")\n",
    "plt.xlabel(\"Ratings\")  # Customize the x-axis label\n",
    "plt.ylabel(\"Count\")    # Customize the y-axis label\n",
    "\n",
    "# Add number on top of each bar\n",
    "for p in ax_fake_reviews.patches:\n",
    "    height = p.get_height()\n",
    "    # Only annotate if the height is greater than 0\n",
    "    if height > 0:\n",
    "        ax_fake_reviews.annotate(f'{height}', \n",
    "                                (p.get_x() + p.get_width() / 2., height), \n",
    "                                ha='center', va='baseline',\n",
    "                                fontsize=8, color='black', xytext=(0, 5), \n",
    "                                textcoords='offset points')\n",
    "\n",
    "# Show legend outside the plot\n",
    "plt.legend(title='Ratings', loc='upper left', bbox_to_anchor=(1, 1))  # Adjust position here\n",
    "\n",
    "# Create a pie chart for the percentage distribution of ratings\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "rating_counts_fake_reviews = fake_reviews_df['rating'].value_counts()\n",
    "wedges_fake_reviews, texts_fake_reviews, autotexts_fake_reviews = plt.pie(\n",
    "    rating_counts_fake_reviews, \n",
    "    autopct='%1.1f%%', \n",
    "    startangle=140, \n",
    "    colors=[rating_colors_fake_reviews[rating] for rating in rating_counts_fake_reviews.index]\n",
    ")\n",
    "\n",
    "# Add title for the pie chart\n",
    "plt.title(\"Percentage Distribution of Ratings (Fake Reviews Dataset)\")\n",
    "\n",
    "# Create a legend with the corresponding colors\n",
    "plt.legend(wedges_fake_reviews, rating_counts_fake_reviews.index, title=\"Ratings\", loc=\"best\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot for Yelp Dataset countplot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Choose a color palette from Set1 with a fixed number of colors\n",
    "set1_colors = sns.color_palette(\"Set1\", n_colors=5)\n",
    "\n",
    "# Define a mapping of ratings to colors from the Set1 palette\n",
    "rating_colors_yelp = {\n",
    "    1.0: set1_colors[0],  # First color (Red)\n",
    "    2.0: set1_colors[1],  # Second color (Blue)\n",
    "    3.0: set1_colors[2],  # Third color (Green)\n",
    "    4.0: set1_colors[3],  # Fourth color (Orange)\n",
    "    5.0: set1_colors[4]   # Fifth color (Yellow)\n",
    "}\n",
    "\n",
    "# Plot the countplot with hue set to 'Rating'\n",
    "ax_yelp = plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "sns.countplot(x='Rating', data=yelp_df, hue='Rating', palette=rating_colors_yelp)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of Ratings (Yelp Dataset)\")\n",
    "plt.xlabel(\"Ratings\")  # Customize the x-axis label\n",
    "plt.ylabel(\"Count\")    # Customize the y-axis label\n",
    "\n",
    "# Add number on top of each bar\n",
    "for p in ax_yelp.patches:\n",
    "    height = p.get_height()\n",
    "    # Only annotate if the height is greater than 0\n",
    "    if height > 0:\n",
    "        ax_yelp.annotate(f'{height}', \n",
    "                         (p.get_x() + p.get_width() / 2., height), \n",
    "                         ha='center', va='baseline',\n",
    "                         fontsize=8, color='black', xytext=(0, 5), \n",
    "                         textcoords='offset points')\n",
    "\n",
    "# Show legend outside the plot\n",
    "plt.legend(title='Ratings', loc='upper left', bbox_to_anchor=(1, 1))  # Adjust position here\n",
    "\n",
    "# Create a pie chart for the percentage distribution of ratings\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "rating_counts_yelp = yelp_df['Rating'].value_counts()\n",
    "wedges_yelp, texts_yelp, autotexts_yelp = plt.pie(\n",
    "    rating_counts_yelp, \n",
    "    autopct='%1.1f%%', \n",
    "    startangle=140, \n",
    "    colors=[rating_colors_yelp[rating] for rating in rating_counts_yelp.index]\n",
    ")\n",
    "\n",
    "# Add title for the pie chart\n",
    "plt.title(\"Percentage Distribution of Ratings (Yelp Dataset)\")\n",
    "\n",
    "# Create a legend with the corresponding colors\n",
    "plt.legend(wedges_yelp, rating_counts_yelp.index, title=\"Ratings\", loc=\"best\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a color palette for fake reviews dataset\n",
    "unique_labels_fake_reviews = fake_reviews_df['label'].unique()\n",
    "colors_fake_review_label = sns.color_palette(\"Set1\", n_colors=len(unique_labels_fake_reviews))\n",
    "\n",
    "# Create a dictionary to map each label to its corresponding color\n",
    "label_color_map_fake_reviews = dict(zip(unique_labels_fake_reviews, colors_fake_review_label))\n",
    "\n",
    "# Set up the plot for Fake Reviews label countplot and pie chart\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot the countplot with hue for fake reviews\n",
    "ax_fake_reviews = plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "sns.countplot(x='label', data=fake_reviews_df, hue='label', palette=label_color_map_fake_reviews)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of Labels (Fake Reviews Dataset)\")\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Label Classification\")  # Customize the x-axis label\n",
    "plt.ylabel(\"Count\")                 # Customize the y-axis label\n",
    "\n",
    "# Add number on top of each bar\n",
    "for p in ax_fake_reviews.patches:\n",
    "    height = p.get_height()\n",
    "    # Only annotate if the height is greater than 0\n",
    "    if height > 0:\n",
    "        ax_fake_reviews.annotate(f'{height}', \n",
    "                                (p.get_x() + p.get_width() / 2., height), \n",
    "                                ha='center', va='baseline',\n",
    "                                fontsize=8, color='black', xytext=(0, 5), \n",
    "                                textcoords='offset points')\n",
    "\n",
    "# Manually create legend for the countplot\n",
    "handles_fake_reviews = [plt.Line2D([0], [0], marker='o', color='w', label=label, \n",
    "                       markerfacecolor=label_color_map_fake_reviews[label]) for label in unique_labels_fake_reviews]\n",
    "plt.legend(handles=handles_fake_reviews, title='Label Classification', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Create a pie chart for the percentage distribution of labels\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "label_counts = fake_reviews_df['label'].value_counts()\n",
    "wedges_fake_review, texts_fake_review, autotexts_fake_review = plt.pie(\n",
    "    label_counts, \n",
    "    autopct='%1.1f%%', \n",
    "    startangle=140, \n",
    "    colors=[label_color_map_fake_reviews[label] for label in label_counts.index]\n",
    ")\n",
    "\n",
    "# Add title for the pie chart\n",
    "plt.title(\"Percentage Distribution of Labels (Fake Reviews Dataset)\")\n",
    "\n",
    "# Create a legend with the corresponding colors for the pie chart\n",
    "plt.legend(wedges_fake_review, label_counts.index, title=\"Label Classification\", loc=\"best\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a color palette for Yelp dataset\n",
    "unique_labels_yelp = yelp_df['Spam(1) and Not Spam(0)'].unique()\n",
    "colors_yelp_label = sns.color_palette(\"Set1\", n_colors=len(unique_labels_yelp))\n",
    "\n",
    "# Create a dictionary to map each label to its corresponding color\n",
    "label_color_map_yelp = dict(zip(unique_labels_yelp, colors_yelp_label))\n",
    "\n",
    "# Set up the plot for Yelp dataset countplot and pie chart\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot the countplot with hue for Yelp dataset\n",
    "ax_yelp = plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "sns.countplot(x='Spam(1) and Not Spam(0)', data=yelp_df, hue='Spam(1) and Not Spam(0)', palette=label_color_map_yelp)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of Spam and Not Spam (Yelp Dataset)\")\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel(\"Spam Classification\")  # Customize the x-axis label\n",
    "plt.ylabel(\"Count\")                 # Customize the y-axis label\n",
    "\n",
    "# Add number on top of each bar\n",
    "for p in ax_yelp.patches:\n",
    "    height = p.get_height()\n",
    "    # Only annotate if the height is greater than 0\n",
    "    if height > 0:\n",
    "        ax_yelp.annotate(f'{height}', \n",
    "                         (p.get_x() + p.get_width() / 2., height), \n",
    "                         ha='center', va='baseline',\n",
    "                         fontsize=8, color='black', xytext=(0, 5), \n",
    "                         textcoords='offset points')\n",
    "\n",
    "# Manually create legend for the countplot\n",
    "handles_yelp = [plt.Line2D([0], [0], marker='o', color='w', label=str(label), \n",
    "                             markerfacecolor=label_color_map_yelp[label]) for label in unique_labels_yelp]\n",
    "plt.legend(handles=handles_yelp, title='Spam Classification', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Create a pie chart for the percentage distribution of spam labels\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "spam_counts = yelp_df['Spam(1) and Not Spam(0)'].value_counts()\n",
    "wedges_yelp, texts_yelp, autotexts_yelp = plt.pie(\n",
    "    spam_counts, \n",
    "    autopct='%1.1f%%', \n",
    "    startangle=140, \n",
    "    colors=[label_color_map_yelp[label] for label in spam_counts.index]\n",
    ")\n",
    "\n",
    "# Add title for the pie chart\n",
    "plt.title(\"Percentage Distribution of Spam and Not Spam (Yelp Dataset)\")\n",
    "\n",
    "# Create a legend with the corresponding colors for the pie chart\n",
    "plt.legend(wedges_yelp, spam_counts.index, title=\"Spam Classification\", loc=\"best\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs between ratings and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of ratings by target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot for both datasets\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a countplot for Fake Reviews\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "sns.countplot(x='rating', hue='label', data=fake_reviews_df, palette='Set1')\n",
    "plt.title(\"Distribution of Ratings by Target Variable (Fake Reviews Dataset)\")\n",
    "plt.xlabel(\"Ratings\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title='Label Classification')\n",
    "\n",
    "# Add annotations for Fake Reviews, only if height > 0\n",
    "for p in plt.gca().patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        plt.annotate(f'{height}', \n",
    "                     (p.get_x() + p.get_width() / 2., height), \n",
    "                     ha='center', va='baseline', fontsize=6, \n",
    "                     color='black', xytext=(0, 5), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "# Create a countplot for Yelp Dataset\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "sns.countplot(x='Rating', hue='Spam(1) and Not Spam(0)', data=yelp_df, palette='Set1')\n",
    "plt.title(\"Distribution of Ratings by Target Variable (Yelp Dataset)\")\n",
    "plt.xlabel(\"Ratings\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title='Spam Classification')\n",
    "\n",
    "# Add annotations for Yelp Dataset, only if height > 0\n",
    "for p in plt.gca().patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        plt.annotate(f'{height}', \n",
    "                     (p.get_x() + p.get_width() / 2., height), \n",
    "                     ha='center', va='baseline', fontsize=8, \n",
    "                     color='black', xytext=(0, 5), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of target variable by ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot for both datasets\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a countplot for Fake Reviews by Target Variable\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "sns.countplot(x='label', hue='rating', data=fake_reviews_df, palette='Set1')\n",
    "plt.title(\"Distribution of Target Variable by Ratings (Fake Reviews Dataset)\")\n",
    "plt.xlabel(\"Label Classification\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title='Ratings')\n",
    "\n",
    "# Add annotations for Fake Reviews, only if height > 0\n",
    "for p in plt.gca().patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        plt.annotate(f'{height}', \n",
    "                     (p.get_x() + p.get_width() / 2., height), \n",
    "                     ha='center', va='baseline', fontsize=8, \n",
    "                     color='black', xytext=(0, 5), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "# Create a countplot for Yelp Dataset by Target Variable\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "sns.countplot(x='Spam(1) and Not Spam(0)', hue='Rating', data=yelp_df, palette='Set1')\n",
    "plt.title(\"Distribution of Target Variable by Ratings (Yelp Dataset)\")\n",
    "plt.xlabel(\"Spam Classification\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title='Ratings')\n",
    "\n",
    "# Add annotations for Yelp Dataset, only if height > 0\n",
    "for p in plt.gca().patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        plt.annotate(f'{height}', \n",
    "                     (p.get_x() + p.get_width() / 2., height), \n",
    "                     ha='center', va='baseline', fontsize=8, \n",
    "                     color='black', xytext=(0, 5), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other distributions for Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs between category and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a color palette for fake reviews\n",
    "colors_fake_review_category = sns.color_palette(\"Set1\", n_colors=fake_reviews_df['category'].nunique())\n",
    "\n",
    "# Count occurrences of each category\n",
    "category_counts = fake_reviews_df['category'].value_counts()\n",
    "\n",
    "# Create a bar plot using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(category_counts.index, category_counts.values, color=colors_fake_review_category)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution of Categories (Fake Reviews Dataset)\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add number on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Create a legend\n",
    "# Use a list comprehension to create labels\n",
    "legend_labels = category_counts.index\n",
    "plt.legend(bars, legend_labels, title=\"Categories\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of category by target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot for Distribution of Category by Target Variable\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a countplot for Category by Target Variable\n",
    "sns.countplot(x='category', hue='label', data=fake_reviews_df, palette='Set1')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution of Categories by Target Variable (Fake Reviews Dataset)\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add annotations for the countplot, only if height > 0\n",
    "for p in plt.gca().patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        plt.annotate(f'{height}', \n",
    "                     (p.get_x() + p.get_width() / 2., height), \n",
    "                     ha='center', va='baseline', fontsize=6, \n",
    "                     color='black', xytext=(0, 5), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Create a legend outside the plot\n",
    "plt.legend(title='Label Classification', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of target variable by category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot for Distribution of Target Variable by Category\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a countplot for Target Variable by Category\n",
    "sns.countplot(x='label', hue='category', data=fake_reviews_df, palette='Set1')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution of Target Variable by Categories (Fake Reviews Dataset)\")\n",
    "plt.xlabel(\"Label Classification\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add annotations for the countplot, only if height > 0\n",
    "for p in plt.gca().patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        plt.annotate(f'{height}', \n",
    "                     (p.get_x() + p.get_width() / 2., height), \n",
    "                     ha='center', va='baseline', fontsize=6, \n",
    "                     color='black', xytext=(0, 5), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Create a legend outside the plot\n",
    "plt.legend(title='Categories', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other distributions for Yelp dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs between reviews and date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of reviews by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the date\n",
    "yelp_df['Year'] = yelp_df['Date'].dt.year\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a countplot for reviews by year\n",
    "sns.countplot(x='Year', data=yelp_df, palette='Set1', hue='Year')  # Set hue=None to avoid the warning\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution of Reviews by Year (Yelp Dataset)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add annotations for the countplot, only if height > 0\n",
    "for p in plt.gca().patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        plt.annotate(f'{height}', \n",
    "                     (p.get_x() + p.get_width() / 2., height), \n",
    "                     ha='center', va='baseline', fontsize=8, \n",
    "                     color='black', xytext=(0, 5), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of reviews by year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Year and Month from the 'Date' column\n",
    "yelp_df['YearMonth'] = yelp_df['Date'].dt.to_period('M')\n",
    "\n",
    "# Count the number of reviews per month-year\n",
    "reviews_per_month = yelp_df['YearMonth'].value_counts().sort_index()\n",
    "\n",
    "# Convert index back to a DatetimeIndex for plotting\n",
    "reviews_per_month.index = reviews_per_month.index.to_timestamp()\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=reviews_per_month.index, y=reviews_per_month.values, marker='o')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution of Reviews by Year-Month (Yelp Dataset)\")\n",
    "plt.xlabel(\"Year-Month\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "\n",
    "# Annotate each point with the review count\n",
    "for x, y in zip(reviews_per_month.index, reviews_per_month.values):\n",
    "    plt.text(x, y, str(y), fontsize=8, ha='center', va='bottom')\n",
    "\n",
    "# Format x-axis to display Year-Month\n",
    "plt.xticks(reviews_per_month.index, reviews_per_month.index.strftime('%Y-%m'), rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs between date and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of year by target variable and Distribution of target variable by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and subplots for side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# First Plot: Distribution of Year by Target Variable\n",
    "sns.countplot(x='Year', hue='Spam(1) and Not Spam(0)', data=yelp_df, palette='Set1', ax=axes[0])\n",
    "\n",
    "# Add title and labels for the first plot\n",
    "axes[0].set_title(\"Distribution of Year by Target Variable (Yelp Dataset)\")\n",
    "axes[0].set_xlabel(\"Year\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Add annotations for the first plot, only if height > 0\n",
    "for p in axes[0].patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        axes[0].annotate(f'{height}', \n",
    "                         (p.get_x() + p.get_width() / 2., height), \n",
    "                         ha='center', va='baseline', fontsize=6, \n",
    "                         color='black', xytext=(0, 5), \n",
    "                         textcoords='offset points')\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Create a legend outside the first plot\n",
    "axes[0].legend(title='Spam Classification', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Second Plot: Distribution of Target Variable by Year\n",
    "sns.countplot(x='Spam(1) and Not Spam(0)', hue='Year', data=yelp_df, palette='Set1', ax=axes[1])\n",
    "\n",
    "# Add title and labels for the second plot\n",
    "axes[1].set_title(\"Distribution of Target Variable by Year (Yelp Dataset)\")\n",
    "axes[1].set_xlabel(\"Spam Classification\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Add annotations for the second plot, only if height > 0\n",
    "for p in axes[1].patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        axes[1].annotate(f'{height}', \n",
    "                         (p.get_x() + p.get_width() / 2., height), \n",
    "                         ha='center', va='baseline', fontsize=6, \n",
    "                         color='black', xytext=(0, 5), \n",
    "                         textcoords='offset points')\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Create a legend outside the second plot\n",
    "axes[1].legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Adjust layout for both plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs between sentiment and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values in 'Sentiment'\n",
    "unique_sentiments = yelp_df['Sentiment'].unique()\n",
    "\n",
    "# Create the countplot without 'palette'\n",
    "ax_yelp = sns.countplot(x='Sentiment', data=yelp_df)\n",
    "\n",
    "# Manually color each bar using the 'Set1' palette (3 unique colors)\n",
    "colors_yelp_sentiment = sns.color_palette(\"Set1\", n_colors=3)\n",
    "for i, bar in enumerate(ax_yelp.patches):\n",
    "    bar.set_color(colors_yelp_sentiment[i % 3])  # Apply colors cyclically for each sentiment\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution of Sentiment in Yelp Dataset\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Annotate each bar with the count\n",
    "for p in ax_yelp.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax_yelp.annotate(f'{height}', \n",
    "                         (p.get_x() + p.get_width() / 2., height), \n",
    "                         ha='center', va='baseline',\n",
    "                         fontsize=8, color='black', xytext=(0, 5), \n",
    "                         textcoords='offset points')\n",
    "\n",
    "# Manually create the legend\n",
    "handles = [mpatches.Patch(color=colors_yelp_sentiment[i], label=unique_sentiments[i]) for i in range(3)]\n",
    "plt.legend(handles=handles, title='Sentiment', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of sentiment by target variable and Distribution of target variable by sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and subplots for side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# First Plot: Distribution of Sentiment by Target Variable\n",
    "sns.countplot(x='Sentiment', hue='Spam(1) and Not Spam(0)', data=yelp_df, palette='Set1', ax=axes[0])\n",
    "\n",
    "# Add title and labels for the first plot\n",
    "axes[0].set_title(\"Distribution of Sentiment by Target Variable (Yelp Dataset)\")\n",
    "axes[0].set_xlabel(\"Sentiment\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Add annotations for the first plot, only if height > 0\n",
    "for p in axes[0].patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        axes[0].annotate(f'{height}', \n",
    "                         (p.get_x() + p.get_width() / 2., height), \n",
    "                         ha='center', va='baseline', fontsize=6, \n",
    "                         color='black', xytext=(0, 5), \n",
    "                         textcoords='offset points')\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Create a legend outside the first plot\n",
    "axes[0].legend(title='Spam Classification', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Second Plot: Distribution of Target Variable by Sentiment\n",
    "sns.countplot(x='Spam(1) and Not Spam(0)', hue='Sentiment', data=yelp_df, palette='Set1', ax=axes[1])\n",
    "\n",
    "# Add title and labels for the second plot\n",
    "axes[1].set_title(\"Distribution of Target Variable by Sentiment (Yelp Dataset)\")\n",
    "axes[1].set_xlabel(\"Spam Classification\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Add annotations for the second plot, only if height > 0\n",
    "for p in axes[1].patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        axes[1].annotate(f'{height}', \n",
    "                         (p.get_x() + p.get_width() / 2., height), \n",
    "                         ha='center', va='baseline', fontsize=6, \n",
    "                         color='black', xytext=(0, 5), \n",
    "                         textcoords='offset points')\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Create a legend outside the second plot\n",
    "axes[1].legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Adjust layout for both plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess_text:\n",
    "\n",
    "1. Converting to Lowercasing\n",
    "2. Removing Emojis\n",
    "3. Normalize Contractions\n",
    "4. Removing Punctuations\n",
    "5. Removal of Links\n",
    "6. Removing Special Characters\n",
    "7. Removing Mentions\n",
    "8. Removing Line breakers\n",
    "9. Removal of UTF-encoding\n",
    "10. Removing Hashtags\n",
    "11. Removing Special Characters\n",
    "12. Removing Extra Whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatize_and_remove_stopwords:\n",
    "1. Removing Stopwords\n",
    "2. Tokenization\n",
    "3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis from the text.\"\"\"\n",
    "    return demoji.replace(text, '')\n",
    "\n",
    "def clean_hashtags(review):\n",
    "    \"\"\"Remove hashtags from the text.\"\"\"\n",
    "    # Remove hashtags at the end of the sentence\n",
    "    new_review = \" \".join(word.strip() for word in re.split(r'#(?!(?:hashtag)\\b)[\\w\\'-]+(?=(?:\\s+#[\\w\\'-]+)*\\s*$)', review))\n",
    "    # Remove '#' symbol from words in the middle of the sentence\n",
    "    new_review2 = \" \".join(word.strip() for word in re.split(r'#|_', new_review))\n",
    "    return new_review2\n",
    "\n",
    "def filter_chars(a):\n",
    "    \"\"\"Filter out unwanted characters from the text.\"\"\"\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def remove_mult_spaces(text):\n",
    "    \"\"\"Remove multiple spaces in the text.\"\"\"\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)\n",
    "\n",
    "def normalize_contractions(text):\n",
    "    \"\"\"Normalize common English contractions.\"\"\"\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"i'm\": \"I am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"i've\": \"I have\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"would've\": \"would have\",  \n",
    "        \"i'd\": \"I would\",           \n",
    "        \"i'll\": \"I will\",           \n",
    "        \"there's\": \"there is\"       \n",
    "    }\n",
    "    # Replace contractions in the text\n",
    "    for contraction, full in contractions.items():\n",
    "        text = text.replace(contraction, full)\n",
    "    return text\n",
    "\n",
    "def normalize_punctuation(text):\n",
    "    \"\"\"Replace multiple consecutive periods and punctuation marks with a single instance and ensure spaces.\"\"\"\n",
    "    text = re.sub(r'\\.{2,}', '.', text)  # Normalize ellipses\n",
    "    text = re.sub(r'([!?.])\\1+', r'\\1', text)  # Normalize multiple punctuation marks\n",
    "    # Add space after punctuation if not at the end of the string\n",
    "    text = re.sub(r'([!?.])(?=\\S)', r'\\1 ', text)  \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text: str, remove_stopwords: bool = True) -> str:\n",
    "    \"\"\"Preprocess the input text.\"\"\"\n",
    "\n",
    "# Check if the input is an integer, float, or contains only numeric characters\n",
    "    if isinstance(text, (int, float)) or (isinstance(text, str) and text.replace('.', '', 1).isdigit()):\n",
    "        return \"\"  # Return an empty string or any placeholder you'd like\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove emojis \n",
    "    text = remove_emojis(text)\n",
    "    # Normalize contractions\n",
    "    text = normalize_contractions(text)\n",
    "    # Normalize punctuation\n",
    "    text = normalize_punctuation(text)\n",
    "    text = re.sub(r'[!.,?;:]', '', text)  \n",
    "    # Remove double quotes\n",
    "    text = re.sub(r'\"', '', text)\n",
    "    # Normalize dashes\n",
    "    text = re.sub(r'-{2,}', '-', text)  # Replace multiple dashes with a single dash\n",
    "    text = re.sub(r'\\s*-\\s*', ' ', text)  # Remove isolated dashes\n",
    "    # Remove links\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "    # Remove specific special characters\n",
    "    text = re.sub(r'[\\\\/ร\\^\\]:,.\\[รท]', '', text) \n",
    "    # Remove all spaces and line breaks\n",
    "    text = text.replace('\\r', '').replace('\\n', '')\n",
    "    # Remove UTF encodings\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text) \n",
    "    # Clean hashtags\n",
    "    text = clean_hashtags(text)\n",
    "    # Filter special characters\n",
    "    text = filter_chars(text)\n",
    "    # Remove multiple spaces\n",
    "    text = remove_mult_spaces(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lemmatize_and_remove_stopwords(text, remove_stopwords: bool = True):\n",
    "    \"\"\"Lemmatize the text and remove stopwords.\"\"\"\n",
    "    special_characters = r'[@_!#$%^&*()<>?/\\|}{~:]'\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [w for w in tokens if (not all(c in special_characters for c in w)) and (not w.isdigit())]\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "        tokens = [w.lower().strip() for w in tokens if w.lower() not in STOPWORDS]\n",
    "        return ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'text_' column in fake_reviews_df\n",
    "fake_reviews_df['cleaned_text'] = fake_reviews_df['text_'].apply(preprocess_text)\n",
    "\n",
    "fake_reviews_df['final_cleaned_review'] = fake_reviews_df['cleaned_text'].apply(lemmatize_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_cleaned_output_path = '/Users/ShanShan/Fake-Reviews-Detection/ShanShan_notebooks/output/processed_fake_reviews.csv'\n",
    "\n",
    "# Save to CSV in the specified output directory\n",
    "fake_reviews_df.to_csv(fake_reviews_cleaned_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'Review' column and convert to string\n",
    "yelp_df['cleaned_review'] = yelp_df['Review'].apply(preprocess_text)\n",
    "\n",
    "yelp_df['final_cleaned_review'] = yelp_df['cleaned_review'].apply(lemmatize_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_cleaned_output_path = '/Users/ShanShan/Fake-Reviews-Detection/ShanShan_notebooks/output/processed_yelp.csv'\n",
    "\n",
    "# Save to CSV in the specified output directory\n",
    "yelp_df.to_csv(yelp_cleaned_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading processed datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "fake_reviews_cleaned_df = pd.read_csv(fake_reviews_cleaned_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_cleaned_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "yelp_cleaned_df = pd.read_csv(yelp_cleaned_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_cleaned_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "fake_reviews_cleaned_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all rows where any column has NA values\n",
    "na_rows_fake_reviews_cleaned= fake_reviews_cleaned_df[fake_reviews_cleaned_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_rows_fake_reviews_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any NA values from the DataFrame\n",
    "fake_reviews_cleaned_new_df = fake_reviews_cleaned_df.copy()\n",
    "\n",
    "fake_reviews_cleaned_new_df = fake_reviews_cleaned_new_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_cleaned_new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "yelp_cleaned_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all rows where any column has NA values\n",
    "na_rows_yelp_cleaned = yelp_cleaned_df[yelp_cleaned_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_rows_yelp_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any NA values from the DataFrame\n",
    "yelp_cleaned_new_df = yelp_cleaned_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_cleaned_new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of unique words\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count punctuation and special characters\n",
    "def count_punctuation_special(text):\n",
    "    # Find all punctuation and special characters\n",
    "    punct_and_special = re.findall(r'[^\\w\\s]', text)\n",
    "    return len(punct_and_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count uppercase characters\n",
    "def count_uppercase_chars(text):\n",
    "    # Find all uppercase letters in the text\n",
    "    uppercase_chars = re.findall(r'[A-Z]', text)\n",
    "    return len(uppercase_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count: counts the number of tokens in the text (separated by a space)\n",
    "fake_reviews_cleaned_new_df['word_count'] = fake_reviews_cleaned_new_df['cleaned_text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# Character count: sum the number of characters of each token\n",
    "fake_reviews_cleaned_new_df['char_count'] = fake_reviews_cleaned_new_df['cleaned_text'].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "\n",
    "# Sentence count: count the number of sentences (separated by a period)\n",
    "fake_reviews_cleaned_new_df['sentence_count'] = fake_reviews_cleaned_new_df['text_'].apply(lambda x: len(str(x).split(\".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length: sum of words length divided by the number of words\n",
    "fake_reviews_cleaned_new_df['avg_word_length'] = fake_reviews_cleaned_new_df.apply(\n",
    "    lambda row: row['char_count'] / row['word_count'] if row['word_count'] > 0 else 0, axis=1)\n",
    "\n",
    "# Average sentence length: sum of sentences length divided by the number of sentences\n",
    "fake_reviews_cleaned_new_df['avg_sentence_length'] = fake_reviews_cleaned_new_df.apply(\n",
    "    lambda row: row['word_count'] / row['sentence_count'] if row['sentence_count'] > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique word count: Counts unique words in each review.\n",
    "fake_reviews_cleaned_new_df['unique_word_count'] = fake_reviews_cleaned_new_df['cleaned_text'].apply(count_unique_words)\n",
    "\n",
    "# Ratio of unique words to total words.\n",
    "fake_reviews_cleaned_new_df['unique_vs_words'] = fake_reviews_cleaned_new_df.apply(\n",
    "    lambda row: row['unique_word_count'] / row['word_count'] if row['word_count'] > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords count\n",
    "stopwords_eng = set(stopwords.words('english'))\n",
    "\n",
    "fake_reviews_cleaned_new_df['stopwords_count'] = fake_reviews_cleaned_new_df['cleaned_text'].str.split().apply(lambda i: len(set(i) & stopwords_eng))\n",
    "\n",
    "# Punctuation and special character count\n",
    "fake_reviews_cleaned_new_df['punctuation_special_count'] = fake_reviews_cleaned_new_df['text_'].apply(count_punctuation_special)\n",
    "\n",
    "# Uppercase character count\n",
    "fake_reviews_cleaned_new_df['uppercase_char_count'] = fake_reviews_cleaned_new_df['text_'].apply(count_uppercase_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_cleaned_new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count: counts the number of tokens in the text (separated by a space)\n",
    "yelp_cleaned_new_df['word_count'] = yelp_cleaned_new_df['cleaned_review'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# Character count: sum the number of characters of each token\n",
    "yelp_cleaned_new_df['char_count'] = yelp_cleaned_new_df['cleaned_review'].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "\n",
    "# Sentence count: count the number of sentences (separated by a period)\n",
    "yelp_cleaned_new_df['sentence_count'] = yelp_cleaned_new_df['Review'].apply(lambda x: len(str(x).split(\".\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length: sum of words length divided by the number of words\n",
    "yelp_cleaned_new_df['avg_word_length'] = yelp_cleaned_new_df.apply(\n",
    "    lambda row: row['char_count'] / row['word_count'] if row['word_count'] > 0 else 0, axis=1)\n",
    "\n",
    "# Average sentence length: sum of sentences length divided by the number of sentences\n",
    "yelp_cleaned_new_df['avg_sentence_length'] = yelp_cleaned_new_df.apply(\n",
    "    lambda row: row['word_count'] / row['sentence_count'] if row['sentence_count'] > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique word count: Counts unique words in each review.\n",
    "yelp_cleaned_new_df['unique_word_count'] = yelp_cleaned_new_df['cleaned_review'].apply(count_unique_words)\n",
    "\n",
    "# Ratio of unique words to total words.\n",
    "yelp_cleaned_new_df['unique_vs_words'] = yelp_cleaned_new_df.apply(\n",
    "    lambda row: row['unique_word_count'] / row['word_count'] if row['word_count'] > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords count\n",
    "stopwords_eng = set(stopwords.words('english'))\n",
    "\n",
    "yelp_cleaned_new_df['stopwords_count'] = yelp_cleaned_new_df['cleaned_review'].str.split().apply(lambda i: len(set(i) & stopwords_eng))\n",
    "\n",
    "# Punctuation and special character count\n",
    "yelp_cleaned_new_df['punctuation_special_count'] = yelp_cleaned_new_df['Review'].apply(count_punctuation_special)\n",
    "\n",
    "# Uppercase character count\n",
    "yelp_cleaned_new_df['uppercase_char_count'] = yelp_cleaned_new_df['Review'].apply(count_uppercase_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of rows with empty tweets.\n",
    "new_train_data = train_data[train_data[\"char_count\"] > 0]\n",
    "new_test_data = test_data[test_data[\"char_count\"] > 0]\n",
    "\n",
    "new_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs between processed review and target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of target variable by average length of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each review in the fake_reviews_df dataset\n",
    "fake_reviews_df['review_length'] = fake_reviews_df['text_'].apply(len)\n",
    "# Calculate the average length of the review grouped by the label\n",
    "average_review_length_fake = fake_reviews_df.groupby('label')['review_length'].mean().reset_index()\n",
    "\n",
    "# Ensure 'text_' and 'Review' columns contain strings\n",
    "yelp_df['Review_new'] = yelp_df['Review'].astype(str)\n",
    "# Calculate the length of each review in the yelp_df dataset\n",
    "yelp_df['review_length'] = yelp_df['Review_new'].apply(len)\n",
    "# Calculate the average length of the review grouped by the Spam(1) column\n",
    "average_review_length_yelp = yelp_df.groupby('Spam(1) and Not Spam(0)')['review_length'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average review length by label for both datasets\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Fake Reviews dataset\n",
    "ax_fake_reviews = plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='label', y='review_length', data=average_review_length_fake, palette=label_color_map_fake_reviews, hue='label', dodge=False, ax=ax_fake_reviews)\n",
    "plt.title('Distribution of Target Variable by \\n Average Review Length (Fake Reviews Dataset)')\n",
    "plt.xlabel('Label Classification')\n",
    "plt.ylabel('Average Review Length')\n",
    "\n",
    "# Add numbers on top of the bars for Fake Reviews dataset\n",
    "for p in ax_fake_reviews.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        ax_fake_reviews.annotate(f'{format(height, \".2f\")}',\n",
    "                                 (p.get_x() + p.get_width() / 2., height),\n",
    "                                 ha='center', va='baseline',\n",
    "                                 fontsize=8, color='black', xytext=(0, 5),\n",
    "                                 textcoords='offset points')\n",
    "\n",
    "# Manually create legend for Fake Reviews dataset\n",
    "unique_labels_fake_reviews = fake_reviews_df['label'].unique()\n",
    "handles_fake_reviews = [plt.Line2D([0], [0], marker='o', color='w', label=label, markerfacecolor=label_color_map_fake_reviews[label]) for label in unique_labels_fake_reviews]\n",
    "plt.legend(handles=handles_fake_reviews, title='Label Classification', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Yelp dataset\n",
    "ax_yelp_reviews = plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='Spam(1) and Not Spam(0)', y='review_length', data=average_review_length_yelp, palette=label_color_map_yelp, hue='Spam(1) and Not Spam(0)', dodge=False, ax=ax_yelp_reviews)\n",
    "plt.title('Distribution of Target Variable by \\n Average Review Length (Yelp Dataset)')\n",
    "plt.xlabel('Spam Classification')\n",
    "plt.ylabel('Average Review Length')\n",
    "\n",
    "# Add numbers on top of the bars for Yelp dataset\n",
    "for p in ax_yelp_reviews.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:  # Only annotate if height is greater than zero\n",
    "        ax_yelp_reviews.annotate(f'{format(height, \".2f\")}',\n",
    "                                 (p.get_x() + p.get_width() / 2., height),\n",
    "                                 ha='center', va='baseline',\n",
    "                                 fontsize=8, color='black', xytext=(0, 5),\n",
    "                                 textcoords='offset points')\n",
    "\n",
    "# Manually create legend for Yelp dataset\n",
    "unique_labels_yelp_reviews = yelp_df['Spam(1) and Not Spam(0)'].unique()\n",
    "handles_yelp_reviews = [plt.Line2D([0], [0], marker='o', color='w', label=label, markerfacecolor=label_color_map_yelp[label]) for label in unique_labels_yelp_reviews]\n",
    "plt.legend(handles=handles_yelp_reviews, title='Spam Classification', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
